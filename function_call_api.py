import asyncio
import json
import time
import uuid
from typing import List, Dict, Any, AsyncIterator, Optional
from fastapi import FastAPI, HTTPException, status
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from contextlib import AsyncExitStack, asynccontextmanager
from openai import OpenAI
from mcp import ClientSession
from mcp.client.sse import sse_client

# ===================== MCP å®¢æˆ·ç«¯å°è£… =====================

class MCPClient:
    def __init__(self):
        self.exit_stack = AsyncExitStack()
        self.session: Optional[ClientSession] = None
        self.tools: List[Dict[str, Any]] = []

    async def connect(self, url: str):
        if self.session:
            return
        streams = await self.exit_stack.enter_async_context(sse_client(url=url))
        self.session = await self.exit_stack.enter_async_context(ClientSession(*streams))
        await self.session.initialize()
        response = await self.session.list_tools()
        self.tools = self._format_tools(response.tools)
        print(f"âœ… MCP connected: {len(self.tools)} tools loaded.")

    def _format_tools(self, mcp_tools) -> List[Dict[str, Any]]:
        tools = []
        for t in mcp_tools:
            schema = t.inputSchema
            tools.append({
                "type": "function",
                "function": {
                    "name": t.name,
                    "description": t.description,
                    "parameters": schema
                }
            })
        return tools

    async def call_tool(self, name: str, args: Dict[str, Any]) -> str:
        if not self.session:
            raise HTTPException(500, "MCP not connected")
        resp = await self.session.call_tool(name, args)
        if resp.isError:
            msg = resp.content[0].text if resp.content else "Unknown MCP error"
            return json.dumps({"error": msg}, ensure_ascii=False)
        return resp.content[0].text if resp.content else "{}"

    async def close(self):
        await self.exit_stack.aclose()

mcp = MCPClient()

# ===================== ä¸šåŠ¡é€»è¾‘ =====================
@asynccontextmanager
async def lifespan(app: FastAPI):
    print("ğŸš€ FastAPI å¯åŠ¨")
    yield
    print("ğŸ›‘ FastAPI å…³é—­")
    await mcp.close()

app = FastAPI(title="MCP-OpenAI Gateway", version="2.0.0", lifespan=lifespan)

@app.get("/health")
async def health():
    return {"status": "ok", "mcp_connected": bool(mcp.session), "tools": len(mcp.tools)}

# ===================== Login =====================
class LoginRequest(BaseModel):
    user_id: str
    password: str
@app.post("/login")
async def login(req: LoginRequest):
    if req.user_id == "admin" and req.password == "admin":
        return {"message": f"User {req.user_id} logged in successfully."}
    elif req.user_id == "pxx" and req.password == "pxx":
        return {"message": f"User {req.user_id} logged in successfully."}
    raise HTTPException(401, "Invalid credentials")

# ===================== MCP Config =====================
class MCPConnectReq(BaseModel):
    url: str
@app.post("/mcp/connect")
async def connect_mcp(req: MCPConnectReq):
    try:
        await mcp.connect(req.url)
        return {"status": "ok", "tools": mcp.tools}
    except Exception as e:
        raise HTTPException(500, f"MCPè¿æ¥å¤±è´¥: {e}")

@app.get("/mcp/tools")
async def list_tools():
    return {"tools": mcp.tools}


# ===================== Chat Completions =====================
# https://platform.openai.com/docs/guides/streaming-responses?api-mode=responses
# https://platform.openai.com/docs/api-reference/responses-streaming/response/created
@app.post("/v1/chat/completions")
async def chat_completions(request: Dict[str, Any]):
    """ä»¿ OpenAI Responses API çš„ç»Ÿä¸€å…¥å£"""
    if request.get("stream"):
        return StreamingResponse(
            stream_chat(request),
            media_type="text/event-stream"
        )
    else:
        return await complete_chat(request)


import json
from fastapi import HTTPException

async def complete_chat(req: Dict[str, Any]):
    """éæµå¼æ¨ç†ï¼šä¿æŒå…¼å®¹ OpenAI SDK è¿”å›ç»“æ„ï¼Œå¹¶åœ¨è¿”å›ä½“ä¸Šé™„åŠ  `trace` å­—æ®µã€‚"""
    messages = req["messages"]
    model = req["model"]

    trace = []

    for step in range(10):
        #TODO: modelæ˜¯å¹³å°äºŒæ¬¡å­˜å‚¨çš„æ¨¡å‹å+åœ°å€
        client = OpenAI(
            base_url="https://api.siliconflow.cn/v1",
            api_key="sk-fvysgovrkoqorpbqpubqfbvfkwfwqkpthtmvzsvdwfkmwpzz"
        )
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            tools=mcp.tools or None,
            temperature=req.get("temperature", 1.0),
            max_tokens=req.get("max_tokens"),
        )

        # æŠŠ SDK å¯¹è±¡è½¬æˆå­—å…¸ï¼ˆå®‰å…¨å¯åºåˆ—åŒ–ï¼‰
        resp_dict = response.model_dump()
        choice = resp_dict["choices"][0]
        message = choice.get("message", {})

        # è®°å½•æœ¬è½® assistant çš„è¾“å‡ºï¼ˆä½¿ç”¨ dictï¼Œé¿å…éåºåˆ—åŒ–å¯¹è±¡ï¼‰
        trace.append({
            "type": "assistant",
            "step": step,
            "message": message,
            "tool_calls": message.get("tool_calls")
        })

        if not message.get("tool_calls"):
            resp_dict["trace"] = trace
            return resp_dict

        for tc in message["tool_calls"]:
            raw_args = tc.get("function", {}).get("arguments", "")
            try:
                parsed_args = json.loads(raw_args) if isinstance(raw_args, str) else raw_args
            except Exception as e:
                parsed_args = {}
                tool_result = {"status": "error", "message": f"parse arguments failed: {e}"}
            else:
                tool_result = await mcp.call_tool(tc["function"]["name"], parsed_args)

            # è®°å½•å·¥å…·è¿”å›
            trace.append({
                "type": "tool_result",
                "step": step,
                "tool_call_id": tc.get("id"),
                "tool_name": tc.get("function", {}).get("name"),
                "arguments": parsed_args,
                "result": tool_result,
            })

            # æŠŠå·¥å…·ç»“æœæ·»åŠ å› messagesï¼ˆè®©ä¸‹ä¸€è½®æ¨¡å‹çœ‹åˆ°ï¼‰
            messages.append({
                "role": "tool",
                "tool_call_id": tc.get("id"),
                "name": tc.get("function", {}).get("name"),
                "content": json.dumps(tool_result, ensure_ascii=False) if not isinstance(tool_result, str) else tool_result
            })

        # æŠŠ assistant çš„ tool_call å ä½æ¶ˆæ¯ä¹ŸåŠ å…¥ä¸Šä¸‹æ–‡ï¼ˆä¸æ¨¡å‹å¯¹è¯æ ¼å¼ä¸€è‡´ï¼‰
        messages.append({
            "role": "assistant",
            "content": None,
            "tool_calls": message["tool_calls"]
        })

    raise HTTPException(status_code=500, detail="Reached max tool call iterations")


async def stream_chat(req: Dict[str, Any]) -> AsyncIterator[str]:
    """æµå¼ç‰ˆæœ¬"""
    messages = req["messages"]
    model = req["model"]
    completion_id = f"chatcmpl-{uuid.uuid4().hex[:8]}"
    created = int(time.time())

    for i in range(10):
        client = OpenAI(
            base_url="https://api.siliconflow.cn/v1",
            api_key="sk-fvysgovrkoqorpbqpubqfbvfkwfwqkpthtmvzsvdwfkmwpzz"
        )
        stream = client.chat.completions.create(
            model=model,
            messages=messages,
            tools=mcp.tools or None,
            temperature=req.get("temperature", 1.0),
            stream=True
        )

        tool_calls: Dict[int, Any] = {}
        for chunk in stream:
            delta = chunk.choices[0].delta
            yield f"data: {chunk.model_dump_json()}\n\n"

            # æ”¶é›†å·¥å…·è°ƒç”¨
            if delta.tool_calls:
                for tc in delta.tool_calls:
                    idx = tc.index
                    if idx not in tool_calls:
                        tool_calls[idx] = tc
                    else:
                        if tc.function.arguments:
                            tool_calls[idx].function.arguments += tc.function.arguments
        if not tool_calls:
            yield "data: [DONE]\n\n"
            return
        else:
            messages.append({
                "role": "assistant",
                "tool_calls": [tc.dict() for tc in tool_calls.values()]
            })

        # å·¥å…·è°ƒç”¨
        for tc in tool_calls.values():
            try:
                args = json.loads(tc.function.arguments)
            except json.JSONDecodeError:
                print("âš ï¸ å·¥å…·è°ƒç”¨å‚æ•°ä¸å®Œæ•´:", tc.function.arguments)
                args = {}
            result = await mcp.call_tool(tc.function.name, args)
            messages.append({
                "role": "tool",
                "tool_call_id": tc.id,
                "name": tc.function.name,
                "content": result
            })

            # === ä»¥ OpenAI æµæ ¼å¼å‘é€å·¥å…·ç»“æœ ===
            # https://platform.openai.com/docs/guides/function-calling#streaming
            tool_event = {
                "id": completion_id,
                "object": "chat.completion.chunk",
                "created": created,
                "model": model,
                "choices": [
                    {
                        "index": i,
                        "delta": {
                            "role": "tool",
                            "tool_call_id": tc.id,
                            "name": tc.function.name,
                            "content": result,
                        },
                        "finish_reason": None
                    }
                ]
            }
            yield f"data: {json.dumps(tool_event, ensure_ascii=False)}\n\n"

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)