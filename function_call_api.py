import asyncio
import json
import time
import uuid
from typing import List, Dict, Any, AsyncIterator, Optional
import aiohttp
from fastapi import FastAPI, HTTPException, Request, status
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from contextlib import AsyncExitStack, asynccontextmanager
from openai import OpenAI
from mcp import ClientSession
from mcp.client.sse import sse_client

# ===================== MCP å®¢æˆ·ç«¯å°è£… =====================

class MCPClient:
    def __init__(self):
        self.exit_stack = AsyncExitStack()
        self.session: Optional[ClientSession] = None
        self.tools: List[Dict[str, Any]] = []

    async def connect(self, url: str):
        if self.session:
            return
        streams = await self.exit_stack.enter_async_context(sse_client(url=url))
        self.session = await self.exit_stack.enter_async_context(ClientSession(*streams))
        await self.session.initialize()
        response = await self.session.list_tools()
        self.tools = self._format_tools(response.tools)
        print(f"âœ… MCP connected: {len(self.tools)} tools loaded.")

    def _format_tools(self, mcp_tools) -> List[Dict[str, Any]]:
        tools = []
        for t in mcp_tools:
            schema = t.inputSchema
            tools.append({
                "type": "function",
                "function": {
                    "name": t.name,
                    "description": t.description,
                    "parameters": schema
                }
            })
        return tools

    async def call_tool(self, name: str, args: Dict[str, Any]) -> str:
        if not self.session:
            raise HTTPException(500, "MCP not connected")
        resp = await self.session.call_tool(name, args)
        if resp.isError:
            msg = resp.content[0].text if resp.content else "Unknown MCP error"
            return json.dumps({"error": msg}, ensure_ascii=False)
        return resp.content[0].text if resp.content else "{}"

    async def close(self):
        await self.exit_stack.aclose()

mcp = MCPClient()

# ===================== ä¸šåŠ¡é€»è¾‘ =====================
@asynccontextmanager
async def lifespan(app: FastAPI):
    print("ğŸš€ FastAPI å¯åŠ¨")
    yield
    print("ğŸ›‘ FastAPI å…³é—­")
    await mcp.close()

app = FastAPI(title="MCP-OpenAI Gateway", version="2.0.0", lifespan=lifespan)

@app.get("/health")
async def health():
    return {"status": "ok", "mcp_connected": bool(mcp.session), "tools": len(mcp.tools)}

# ===================== Login =====================
class LoginRequest(BaseModel):
    user_id: str
    password: str
@app.post("/login")
async def login(req: LoginRequest):
    if req.user_id == "admin" and req.password == "admin":
        return {"message": f"User {req.user_id} logged in successfully."}
    elif req.user_id == "pxx" and req.password == "pxx":
        return {"message": f"User {req.user_id} logged in successfully."}
    raise HTTPException(401, "Invalid credentials")

# ===================== MCP Config =====================
class MCPConnectReq(BaseModel):
    url: str
@app.post("/mcp/connect")
async def connect_mcp(req: MCPConnectReq):
    try:
        await mcp.connect(req.url)
        return {"status": "ok", "tools": mcp.tools}
    except Exception as e:
        raise HTTPException(500, f"MCPè¿æ¥å¤±è´¥: {e}")

@app.get("/mcp/tools")
async def list_tools():
    return {"tools": mcp.tools}


# ===================== Chat Completions =====================
# https://platform.openai.com/docs/guides/streaming-responses?api-mode=responses
# https://platform.openai.com/docs/api-reference/responses-streaming/response/created  
@app.post("/v1/chat/completions")
async def chat_completions(req: Request):
    body = await req.json()
    if body.get("stream"):
        async def safe_stream():
            try:
                async for chunk in stream_chat(body):
                    if await req.is_disconnected():
                        print("âš ï¸ å®¢æˆ·ç«¯æ–­å¼€è¿æ¥ï¼Œåœæ­¢æ¨é€ã€‚")
                        break
                    yield chunk
            except Exception as e:
                print(f"âŒ æµå¼å¼‚å¸¸: {e}")

        return StreamingResponse(safe_stream(), media_type="text/event-stream")

    return await complete_chat(body)


import json
from fastapi import HTTPException

def make_chunk(id, model, created, role, index=0, content=None, annotations=None, tool_call_id=None):
    delta = {"role": role}
    if content is not None: delta["content"] = content
    if annotations is not None: delta["annotations"] = annotations
    if tool_call_id: delta["tool_call_id"] = tool_call_id
    return f"data: {json.dumps({'id': id,'object':'chat.completion.chunk','created':created,'model':model,'choices':[{'index':index,'delta':delta,'finish_reason':None}]}, ensure_ascii=False)}\n\n"


# ============ ğŸ§© RAG æŸ¥è¯¢å‡½æ•° ============ #
async def rag_retrieve(query: str, extra: Dict[str, Any]) -> Dict[str, Any]:
    """æœ€ç®€ç‰ˆ RAG æ£€ç´¢å‡½æ•°ï¼Œç›´æ¥ä¼ å…¥ query å’Œ extra"""
    file_search = extra.get("file_search")
    if not file_search:
        return {"results": []}

    base_url = "http://localhost:8900"   # TODO: æ”¹ä¸ºå®é™…æ£€ç´¢æœåŠ¡åœ°å€
    vector_store_ids = file_search.get("vector_store_ids", [])
    project_id = vector_store_ids[0] if vector_store_ids else "default"
    payload = {
        "query": query,
        "retrieval_mode": "hybrid",
        "top_k": file_search.get("max_num_results", 5),
        "retrieval_weight": 0.5,
    }

    url = f"{base_url}/querySimple/{project_id}"
    headers = {"Content-Type": "application/json"}

    async with aiohttp.ClientSession() as session:
        async with session.post(url, json=payload, headers=headers) as resp:
            data = await resp.json()
            if isinstance(data, list):
                # è¿”å›åˆ—è¡¨æ—¶åŒ…è£…æˆå­—å…¸
                results = [{"id": str(i), "filename": "", "content": str(item)} for i, item in enumerate(data)]
            else:
                results = data.get("results", [])
            return {"results": results}
        

async def complete_chat(req: Dict[str, Any]):
    """éæµå¼æ¨ç†ï¼šä¿æŒå…¼å®¹ OpenAI SDK è¿”å›ç»“æ„ï¼Œå¹¶åœ¨è¿”å›ä½“ä¸Šé™„åŠ  `trace` å­—æ®µã€‚"""
    messages = req["messages"]
    model = req["model"]

    trace = []

    for step in range(10):
        #TODO: modelæ˜¯å¹³å°äºŒæ¬¡å­˜å‚¨çš„æ¨¡å‹å+åœ°å€
        client = OpenAI(
            base_url="https://api.siliconflow.cn/v1",
            api_key="sk-fvysgovrkoqorpbqpubqfbvfkwfwqkpthtmvzsvdwfkmwpzz"
        )
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            tools=mcp.tools or None,
            temperature=req.get("temperature", 1.0),
            max_tokens=req.get("max_tokens"),
        )

        # æŠŠ SDK å¯¹è±¡è½¬æˆå­—å…¸ï¼ˆå®‰å…¨å¯åºåˆ—åŒ–ï¼‰
        resp_dict = response.model_dump()
        choice = resp_dict["choices"][0]
        message = choice.get("message", {})

        # è®°å½•æœ¬è½® assistant çš„è¾“å‡ºï¼ˆä½¿ç”¨ dictï¼Œé¿å…éåºåˆ—åŒ–å¯¹è±¡ï¼‰
        trace.append({
            "type": "assistant",
            "step": step,
            "message": message,
            "tool_calls": message.get("tool_calls")
        })

        if not message.get("tool_calls"):
            resp_dict["trace"] = trace
            return resp_dict

        for tc in message["tool_calls"]:
            raw_args = tc.get("function", {}).get("arguments", "")
            try:
                parsed_args = json.loads(raw_args) if isinstance(raw_args, str) else raw_args
            except Exception as e:
                parsed_args = {}
                tool_result = {"status": "error", "message": f"parse arguments failed: {e}"}
            else:
                tool_result = await mcp.call_tool(tc["function"]["name"], parsed_args)

            # è®°å½•å·¥å…·è¿”å›
            trace.append({
                "type": "tool_result",
                "step": step,
                "tool_call_id": tc.get("id"),
                "tool_name": tc.get("function", {}).get("name"),
                "arguments": parsed_args,
                "result": tool_result,
            })

            # æŠŠå·¥å…·ç»“æœæ·»åŠ å› messagesï¼ˆè®©ä¸‹ä¸€è½®æ¨¡å‹çœ‹åˆ°ï¼‰
            messages.append({
                "role": "tool",
                "tool_call_id": tc.get("id"),
                "name": tc.get("function", {}).get("name"),
                "content": json.dumps(tool_result, ensure_ascii=False) if not isinstance(tool_result, str) else tool_result
            })

        # æŠŠ assistant çš„ tool_call å ä½æ¶ˆæ¯ä¹ŸåŠ å…¥ä¸Šä¸‹æ–‡ï¼ˆä¸æ¨¡å‹å¯¹è¯æ ¼å¼ä¸€è‡´ï¼‰
        messages.append({
            "role": "assistant",
            "content": None,
            "tool_calls": message["tool_calls"]
        })

    raise HTTPException(status_code=500, detail="Reached max tool call iterations")


async def stream_chat(req: Dict[str, Any]) -> AsyncIterator[str]:
    """æµå¼ç‰ˆæœ¬"""
    messages = req["messages"]
    model = req["model"]
    completion_id = f"chatcmpl-{uuid.uuid4().hex[:8]}"
    created = int(time.time())

    # ============ ğŸ” RAG æ£€ç´¢é˜¶æ®µ ============ #
    extra = req.get("extra", {}) or {}
    file_search = extra.get("file_search")
    query = req["messages"][-1]["content"]

    if file_search:  # åªæœ‰æœ‰ file_search æ‰æ‰§è¡Œ RAG æŸ¥è¯¢
        try:
            rag_context = await rag_retrieve(query, extra)
            results = rag_context.get("results", [])

            if results:
                # æ‹¼æ¥æ–‡æ¡£å†…å®¹åŠ å…¥ç³»ç»Ÿæ¶ˆæ¯
                concatenated = "\n\n".join([doc.get("content", "") for doc in results])
                req["messages"].append({
                    "role": "system",
                    "content": f"ä»¥ä¸‹æ˜¯ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³çš„å‚è€ƒèµ„æ–™ï¼š\n{concatenated}"
                })
                print("âœ… å·²æ·»åŠ RAGä¸Šä¸‹æ–‡ã€‚")

                # æµå¼è¿”å›æ¯æ¡ RAG æ–‡æ¡£
                for i, doc in enumerate(results):
                    yield make_chunk(
                        completion_id, model, created,
                        role="file_search",
                        index=i,
                        annotations=[{
                            "id": doc.get("id"),
                            "filename": doc.get("filename"),
                            "content": doc.get("content")
                        }]
                    )
        except Exception as e:
            print(f"âš ï¸ RAG æ£€ç´¢å¤±è´¥: {e}")


    for i in range(10):
        client = OpenAI(
            base_url="https://api.siliconflow.cn/v1",
            api_key="sk-fvysgovrkoqorpbqpubqfbvfkwfwqkpthtmvzsvdwfkmwpzz"
        )
        stream = client.chat.completions.create(
            model=model,
            messages=messages,
            tools=mcp.tools or None,
            temperature=req.get("temperature", 1.0),
            stream=True
        )

        tool_calls: Dict[int, Any] = {}
        for chunk in stream:
            delta = chunk.choices[0].delta
            yield f"data: {chunk.model_dump_json()}\n\n"

            # æ”¶é›†å·¥å…·è°ƒç”¨
            if delta.tool_calls:
                for tc in delta.tool_calls:
                    idx = tc.index
                    if idx not in tool_calls:
                        tool_calls[idx] = tc
                    else:
                        if tc.function.arguments:
                            tool_calls[idx].function.arguments += tc.function.arguments
        if not tool_calls:
            yield "data: [DONE]\n\n"
            return
        else:
            messages.append({
                "role": "assistant",
                "tool_calls": [tc.dict() for tc in tool_calls.values()]
            })

        # å·¥å…·è°ƒç”¨
        for tc in tool_calls.values():
            try:
                args = json.loads(tc.function.arguments)
            except json.JSONDecodeError:
                print("âš ï¸ å·¥å…·è°ƒç”¨å‚æ•°ä¸å®Œæ•´:", tc.function.arguments)
                args = {}
            result = await mcp.call_tool(tc.function.name, args)
            messages.append({
                "role": "tool",
                "tool_call_id": tc.id,
                "name": tc.function.name,
                "content": result
            })

            # === ä»¥ OpenAI æµæ ¼å¼å‘é€å·¥å…·ç»“æœ ===
            # https://platform.openai.com/docs/guides/function-calling#streaming
            yield make_chunk(
                completion_id, model, created,
                role="tool",
                index=i,
                content=result,
                tool_call_id=tc.id
            )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)